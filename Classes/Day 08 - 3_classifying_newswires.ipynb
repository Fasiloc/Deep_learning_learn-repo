{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DH9hmjLdLhu8",
        "outputId": "ade8ccce-c03c-441e-8782-8240cbc28125"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdxv2pofLhvE"
      },
      "source": [
        "# Classifying newswires: a multi-class classification example\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGqZ6FwqLhvH"
      },
      "source": [
        "## The Reuters dataset\n",
        "\n",
        "\n",
        "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
        "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
        "topic has at least 10 examples in the training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "collapsed": true,
        "id": "66g98I-eLhvI"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz3EoGuBLhvJ"
      },
      "source": [
        "\n",
        "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
        "data.\n",
        "\n",
        "We have 8,982 training examples and 2,246 test examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZLqqQyJLhvJ",
        "outputId": "7c4c265b-9d09-4b73-df82-bdda8e1b0059"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur32a_bBLhvM",
        "outputId": "1cef480e-b036-4d38-8bad-9069064e7dab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3OO1APMLhvM"
      },
      "source": [
        "As with the IMDB reviews, each example is a list of integers (word indices):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_s83ptaLhvN",
        "outputId": "b16660e6-448d-4ef8-b20c-89d5a7056c2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "train_data[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL4p_9kOLhvO"
      },
      "source": [
        "Here's how you can decode it back to words, in case you are curious:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "collapsed": true,
        "id": "DGcceqXQLhvP"
      },
      "outputs": [],
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "sABneZvbLhvP",
        "outputId": "363226ad-9cb6-42ab-c5ee-a36a706d9dfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "decoded_newswire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q-kvK0GLhvQ"
      },
      "source": [
        "The label associated with an example is an integer between 0 and 45: a topic index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-P2ZNR2LhvR"
      },
      "outputs": [],
      "source": [
        "train_labels[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7GoIGoLhvR"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We can vectorize the data with the exact same code as in our previous example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true,
        "id": "ihXYpD0aLhvR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "collapsed": true,
        "id": "4lMxwREiLhvS"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training labels\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# Our vectorized test labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbBU4kC7LhvT"
      },
      "source": [
        "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mxpw42G9r1pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "collapsed": true,
        "id": "IdllxWjjLhvT"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "collapsed": true,
        "id": "03ncjwrdLhvU"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "collapsed": true,
        "id": "QvBrM52LLhvU"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WsPahkmwLhvV"
      },
      "outputs": [],
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPitNWF7LhvV",
        "outputId": "35e5e83e-73f6-4469-d28b-94c4d694feb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 2s 62ms/step - loss: 2.5571 - accuracy: 0.5370 - val_loss: 1.7132 - val_accuracy: 0.6520\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 1.4080 - accuracy: 0.7071 - val_loss: 1.2948 - val_accuracy: 0.7050\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 1.0318 - accuracy: 0.7799 - val_loss: 1.1122 - val_accuracy: 0.7590\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.8120 - accuracy: 0.8291 - val_loss: 1.0200 - val_accuracy: 0.7820\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.6446 - accuracy: 0.8680 - val_loss: 0.9525 - val_accuracy: 0.8010\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.5131 - accuracy: 0.8936 - val_loss: 0.9261 - val_accuracy: 0.8100\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.4103 - accuracy: 0.9171 - val_loss: 0.9123 - val_accuracy: 0.8150\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3352 - accuracy: 0.9340 - val_loss: 0.9062 - val_accuracy: 0.8080\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.2769 - accuracy: 0.9401 - val_loss: 0.9428 - val_accuracy: 0.7970\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.2365 - accuracy: 0.9450 - val_loss: 0.9163 - val_accuracy: 0.8090\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.2001 - accuracy: 0.9503 - val_loss: 0.9218 - val_accuracy: 0.8180\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.1787 - accuracy: 0.9536 - val_loss: 0.9579 - val_accuracy: 0.8080\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.1596 - accuracy: 0.9524 - val_loss: 0.9633 - val_accuracy: 0.8200\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.1477 - accuracy: 0.9549 - val_loss: 0.9595 - val_accuracy: 0.8140\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.1366 - accuracy: 0.9567 - val_loss: 0.9965 - val_accuracy: 0.8110\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.1230 - accuracy: 0.9583 - val_loss: 1.0304 - val_accuracy: 0.8000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.1213 - accuracy: 0.9570 - val_loss: 1.0945 - val_accuracy: 0.8000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1182 - accuracy: 0.9574 - val_loss: 1.0117 - val_accuracy: 0.8150\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.1138 - accuracy: 0.9589 - val_loss: 1.0734 - val_accuracy: 0.7980\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.1106 - accuracy: 0.9574 - val_loss: 1.0564 - val_accuracy: 0.8080\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SELqY4oYLhvW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "1GXWEWE-M6Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Ip0VMRLhvW"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKJXbeP9LhvW"
      },
      "source": [
        "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
        "the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySNQPTGVLhvX"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diA6K0bPLhvX"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzjsOvMALhvX"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YSUUs8kXLhvY"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW4eCpFDLhvY"
      },
      "source": [
        "Each entry in `predictions` is a vector of length 46:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZzaQzeJLhvY"
      },
      "outputs": [],
      "source": [
        "predictions[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br2AmcwXLhvZ"
      },
      "outputs": [],
      "source": [
        "np.sum(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oVjeVljLhvZ"
      },
      "outputs": [],
      "source": [
        "np.argmax(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eZQyUktgLhvZ"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yGS2fhzELhva"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdDOnpM0Lhvb"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBP2fn3vLhvc"
      },
      "source": [
        "\n",
        "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
        "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
        "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
        "of it."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "3-classifying-newswires.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
