{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rXBfTb8YP-Wt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "import progressbar\n",
        "\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment = gym.make(\"Taxi-v3\").env\n",
        "enviroment.render()\n",
        "\n",
        "print('Number of states: {}'.format(enviroment.observation_space.n))\n",
        "print('Number of actions: {}'.format(enviroment.action_space.n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjBAmjMbRSfE",
        "outputId": "cabfe457-4421-4cac-cd11-a79738d54625"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Number of states: 500\n",
            "Number of actions: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, enviroment, optimizer):\n",
        "        \n",
        "        # Initialize atributes\n",
        "        self._state_size = enviroment.observation_space.n\n",
        "        self._action_size = enviroment.action_space.n\n",
        "        self._optimizer = optimizer\n",
        "        \n",
        "        self.expirience_replay = deque(maxlen=2000)\n",
        "        \n",
        "        # Initialize discount and exploration rate\n",
        "        self.gamma = 0.6\n",
        "        self.epsilon = 0.1\n",
        "        \n",
        "        # Build networks\n",
        "        self.q_network = self._build_compile_model()\n",
        "        self.target_network = self._build_compile_model()\n",
        "        self.alighn_target_model()\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminated):\n",
        "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
        "    \n",
        "    def _build_compile_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self._state_size, 10, input_length=1))\n",
        "        model.add(Reshape((10,)))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(self._action_size, activation='linear'))\n",
        "        \n",
        "        model.compile(loss='mse', optimizer=self._optimizer)\n",
        "        return model\n",
        "\n",
        "    def alighn_target_model(self):\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return enviroment.action_space.sample()\n",
        "        \n",
        "        q_values = self.q_network.predict(state)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def retrain(self, batch_size):\n",
        "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
        "        \n",
        "        for state, action, reward, next_state, terminated in minibatch:\n",
        "            \n",
        "            target = self.q_network.predict(state)\n",
        "            \n",
        "            if terminated:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                t = self.target_network.predict(next_state)\n",
        "                target[0][action] = reward + self.gamma * np.amax(t)\n",
        "            \n",
        "            self.q_network.fit(state, target, epochs=1, verbose=0)"
      ],
      "metadata": {
        "id": "uW6r2wHiRfNl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.01)\n",
        "agent = Agent(enviroment, optimizer)\n",
        "\n",
        "batch_size = 32\n",
        "num_of_episodes = 2\n",
        "timesteps_per_episode = 1000\n",
        "agent.q_network.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aC6bhXjU-4V",
        "outputId": "5f8278d3-f88d-4162-89af-0862686bd7cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1, 10)             5000      \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                550       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 6)                 306       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,406\n",
            "Trainable params: 8,406\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(0, num_of_episodes):\n",
        "    # Reset the enviroment\n",
        "    state = enviroment.reset()\n",
        "    state = np.reshape(state, [1, 1])\n",
        "    \n",
        "    # Initialize variables\n",
        "    reward = 0\n",
        "    terminated = False\n",
        "    \n",
        "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=\\\n",
        "[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    \n",
        "    for timestep in range(timesteps_per_episode):\n",
        "        # Run Action\n",
        "        action = agent.act(state)\n",
        "        \n",
        "        # Take action    \n",
        "        next_state, reward, terminated, info = enviroment.step(action) \n",
        "        next_state = np.reshape(next_state, [1, 1])\n",
        "        agent.store(state, action, reward, next_state, terminated)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if terminated:\n",
        "            agent.alighn_target_model()\n",
        "            break\n",
        "            \n",
        "        if len(agent.expirience_replay) > batch_size:\n",
        "            agent.retrain(batch_size)\n",
        "        \n",
        "        if timestep%10 == 0:\n",
        "            bar.update(timestep/10 + 1)\n",
        "    \n",
        "    bar.finish()\n",
        "    if (e + 1) % 10 == 0:\n",
        "        print(\"**********************************\")\n",
        "        print(\"Episode: {}\".format(e + 1))\n",
        "        enviroment.render()\n",
        "        print(\"**********************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0IBlAseVEjk",
        "outputId": "61e01f70-2eac-450a-f596-8ecb53417290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[========================================================================] 100%\n",
            "[=============================                                           ]  41%"
          ]
        }
      ]
    }
  ]
}