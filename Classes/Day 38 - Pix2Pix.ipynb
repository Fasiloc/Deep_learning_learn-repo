{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Pix2Pix.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1JwoVgHtoqt"
      },
      "outputs": [],
      "source": [
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.models import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_discriminator(image_shape):\n",
        "    \n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02) #As described in the original paper\n",
        "    \n",
        "\t# source image input\n",
        "\tin_src_image = Input(shape=image_shape)  #Image we want to convert to another image\n",
        "\t# target image input\n",
        "\tin_target_image = Input(shape=image_shape)  #Image we want to generate after training. \n",
        "    \n",
        "\t# concatenate images, channel-wise\n",
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
        "    \n",
        "\t# C64: 4x4 kernel Stride 2x2\n",
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C128: 4x4 kernel Stride 2x2\n",
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C256: 4x4 kernel Stride 2x2\n",
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C512: 4x4 kernel Stride 2x2 \n",
        "    # Not in the original paper. Comment this block if you want.\n",
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# second last output layer : 4x4 kernel but Stride 1x1\n",
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# patch output\n",
        "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\tpatch_out = Activation('sigmoid')(d)\n",
        "\t# define model\n",
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
        "\t# compile model\n",
        "    #The model is trained with a batch size of one image and Adam opt. \n",
        "    #with a small learning rate and 0.5 beta. \n",
        "    #The loss for the discriminator is weighted by 50% for each model update.\n",
        "    \n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "Sc8mPHy5tw5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disc_model = define_discriminator((256,256,3))\n",
        "# plot_model(disc_model, to_file='disc_model.png', show_shapes=True)\n",
        "\n",
        "##############################\n",
        "#Now define the generator - in our case we will define a U-net\n",
        "# define an encoder block to be used in generator\n",
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add downsampling layer\n",
        "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# conditionally add batch normalization\n",
        "\tif batchnorm:\n",
        "\t\tg = BatchNormalization()(g, training=True)\n",
        "\t# leaky relu activation\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\treturn g\n"
      ],
      "metadata": {
        "id": "phr1uLD_uMjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a decoder block to be used in generator\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add upsampling layer\n",
        "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# add batch normalization\n",
        "\tg = BatchNormalization()(g, training=True)\n",
        "\t# conditionally add dropout\n",
        "\tif dropout:\n",
        "\t\tg = Dropout(0.5)(g, training=True)\n",
        "\t# merge with skip connection\n",
        "\tg = Concatenate()([g, skip_in])\n",
        "\t# relu activation\n",
        "\tg = Activation('relu')(g)\n",
        "\treturn g\n"
      ],
      "metadata": {
        "id": "NnlYU0SpuOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the standalone generator model - U-net\n",
        "def define_generator(image_shape=(256,256,3)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=image_shape)\n",
        "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n",
        "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "\te2 = define_encoder_block(e1, 128)\n",
        "\te3 = define_encoder_block(e2, 256)\n",
        "\te4 = define_encoder_block(e3, 512)\n",
        "\te5 = define_encoder_block(e4, 512)\n",
        "\te6 = define_encoder_block(e5, 512)\n",
        "\te7 = define_encoder_block(e6, 512)\n",
        "\t# bottleneck, no batch norm and relu\n",
        "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "\tb = Activation('relu')(b)\n",
        "\t# decoder model: CD512-CD512-CD512-C512-C256-C128-C64\n",
        "\td1 = decoder_block(b, e7, 512)\n",
        "\td2 = decoder_block(d1, e6, 512)\n",
        "\td3 = decoder_block(d2, e5, 512)\n",
        "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
        "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
        "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
        "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "\t# output\n",
        "\tg = Conv2DTranspose(image_shape[2], (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7) #Modified \n",
        "\tout_image = Activation('tanh')(g)  #Generates images in the range -1 to 1. So change inputs also to -1 to 1\n",
        "\t# define model\n",
        "\tmodel = Model(in_image, out_image)\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "bnR7iLxcuROW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model, image_shape):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False       #Descriminator layers set to untrainable in the combined GAN but \n",
        "                                                #standalone descriminator will be trainable.\n",
        "            \n",
        "\t# define the source image\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\t# suppy the image as input to the generator \n",
        "\tgen_out = g_model(in_src)\n",
        "\t# supply the input image and generated image as inputs to the discriminator\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\t# src image as input, generated image and disc. output as outputs\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    \n",
        "    #Total loss is the weighted sum of adversarial loss (BCE) and L1 loss (MAE)\n",
        "    #Authors suggested weighting BCE vs L1 as 1:100.\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'], \n",
        "               optimizer=opt, loss_weights=[1,100])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "2ZFL9lpEuYj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset # train A is orginal image , TrainB is mask\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2], y\n"
      ],
      "metadata": {
        "id": "rRw5RTHiubCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tX = g_model.predict(samples)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "\treturn X, y"
      ],
      "metadata": {
        "id": "VSsM-VyYud1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the generator model and check how good the generated image looks. \n",
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "\t# select a sample of input images\n",
        "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "\t# generate a batch of fake samples\n",
        "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "\t# scale all pixels from [-1,1] to [0,1]\n",
        "\tX_realA = (X_realA + 1) / 2.0\n",
        "\tX_realB = (X_realB + 1) / 2.0\n",
        "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
        "\t# plot real source images\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_realA[i])\n",
        "\t# plot generated target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + n_samples + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_fakeB[i])\n",
        "\t# plot real target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_realB[i])\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
        "\tplt.savefig(filename1)\n",
        "\tplt.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))\n"
      ],
      "metadata": {
        "id": "QqBd5akBuhDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train pix2pix models\n",
        "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
        "\t# determine the output square shape of the discriminator\n",
        "\tn_patch = d_model.output_shape[1]\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(len(trainA) / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "\t\t# update discriminator for real samples\n",
        "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "\t\t# update discriminator for generated samples\n",
        "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "\t\t# update the generator\n",
        "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "\t\t# summarize performance\n",
        "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "\t\t# summarize model performance\n",
        "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, dataset)"
      ],
      "metadata": {
        "id": "DhLif7ymCoHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the generator model and check how good the generated image looks. \n",
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "\t# select a sample of input images\n",
        "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "\t# generate a batch of fake samples\n",
        "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "\t# scale all pixels from [-1,1] to [0,1]\n",
        "\tX_realA = (X_realA + 1) / 2.0\n",
        "\tX_realB = (X_realB + 1) / 2.0\n",
        "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
        "\t# plot real source images\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_realA[i])\n",
        "\t# plot generated target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + n_samples + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_fakeB[i])\n",
        "\t# plot real target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_realB[i])\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
        "\tplt.savefig(filename1)\n",
        "\tplt.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))\n"
      ],
      "metadata": {
        "id": "tmvhm_kFujl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from numpy import asarray, load\n",
        "from numpy import vstack\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from numpy import savez_compressed\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "5-rE75oivMkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "a2UqDFniAJM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path='/content/drive/MyDrive/Lecture/LAB/DAY 2/Semantic Segmentation/dataset/data'\n",
        "gt_path='/content/drive/MyDrive/Lecture/LAB/DAY 2/Semantic Segmentation/dataset/groundtruth' \n"
      ],
      "metadata": {
        "id": "WFKqs5I_45AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "iWwDbhgGAieW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "for file in os.scandir(data_path):\n",
        "    gt_im_path=os.path.join(gt_path, \"imgy\"+file.name.split('x')[1]) #splitting the name with x , Train images ./dataset/data/imgx23.jpg, Test Images ./dataset/groundtruth/imgy23.jpg\n",
        "    print(file.path,gt_im_path)\n",
        "    im = cv2.imread(file.path)\n",
        "    im = cv2.resize(im,(256,256))\n",
        "    gt = cv2.imread(gt_im_path)\n",
        "    gt = cv2.resize(gt,(256,256))\n",
        "    X.append(im)\n",
        "    Y.append(gt)\n",
        "  "
      ],
      "metadata": {
        "id": "KvHm1CCc8y65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_images = np.array(X)\n",
        "tar_images = np.array(Y)\n",
        "\n",
        "print(src_images.shape)\n",
        "print(tar_images.shape)"
      ],
      "metadata": {
        "id": "V9aDvkCzAhSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 3\n",
        "for i in range(n_samples):\n",
        "\tpyplot.subplot(2, n_samples, 1 + i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(src_images[i])\n",
        "# plot target image\n",
        "for i in range(n_samples):\n",
        "\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(tar_images[i])\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "OwDPJWjJ6qKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = src_images.shape[1:]\n",
        "image_shape"
      ],
      "metadata": {
        "id": "VjEYdFdZB8oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the models\n",
        "d_model = define_discriminator(image_shape)\n",
        "g_model = define_generator(image_shape)\n",
        "# define the composite model\n",
        "gan_model = define_gan(g_model, d_model, image_shape)"
      ],
      "metadata": {
        "id": "mZR5gYvEB98P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define data\n",
        "# load and prepare training images\n",
        "data = [src_images, tar_images]"
      ],
      "metadata": {
        "id": "6iRKtiTACSgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "\t# load compressed arrays\n",
        "\t# unpack arrays\n",
        "\tX1, X2 = data[0], data[1]\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX1 = (X1 - 127.5) / 127.5\n",
        "\tX2 = (X2 - 127.5) / 127.5\n",
        "\treturn [X1, X2]"
      ],
      "metadata": {
        "id": "yHOGOF5DCVJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = preprocess_data(data)"
      ],
      "metadata": {
        "id": "8c6IMbHLCXJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime \n",
        "start1 = datetime.now() \n",
        "\n",
        "train(d_model, g_model, gan_model, dataset, n_epochs=10, n_batch=1) \n",
        "#Reports parameters for each batch (total 50) for each epoch.\n",
        "#For 10 epochs we should see 500\n",
        "\n",
        "stop1 = datetime.now()\n",
        "#Execution time of the model \n",
        "execution_time = stop1-start1\n",
        "print(\"Execution time is: \", execution_time)\n",
        "\n",
        "#Reports parameters for each batch (total 50) for each epoch.\n",
        "#For 10 epochs we should see 500"
      ],
      "metadata": {
        "id": "8Nd7b6EXCbBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_model.save('saved_model_10epochs.h5')"
      ],
      "metadata": {
        "id": "mvUqomVOIxH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test trained model on a few images...\n",
        "\n",
        "from keras.models import load_model\n",
        "from numpy.random import randint\n",
        "model = load_model('/content/saved_model_10epochs.h5')"
      ],
      "metadata": {
        "id": "DwtJSHWWIjkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot source, generated and target images\n",
        "def plot_images(src_img, gen_img, tar_img):\n",
        "\timages = vstack((src_img, gen_img, tar_img))\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\timages = (images + 1) / 2.0\n",
        "\ttitles = ['Source', 'Generated', 'Expected']\n",
        "\t# plot images row by row\n",
        "\tfor i in range(len(images)):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(1, 3, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(images[i])\n",
        "\t\t# show title\n",
        "\t\tpyplot.title(titles[i])\n",
        "\tpyplot.show()\n"
      ],
      "metadata": {
        "id": "1d_rg5CbIrfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[X1, X2] = dataset\n",
        "# select random example\n",
        "ix = randint(0, len(X1), 1)\n",
        "src_image, tar_image = X1[ix], X2[ix]\n",
        "# generate image from source\n",
        "gen_image = model.predict(src_image)\n",
        "# plot all three images\n",
        "plot_images(src_image, gen_image, tar_image)"
      ],
      "metadata": {
        "id": "viAysDZsJqzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}