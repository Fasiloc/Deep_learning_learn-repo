{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPB/igSSAmbRcTadOwUDGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fasiloc/Deep_learning_learn-repo/blob/main/Classes/Day_45_Text_mining_natural_language_processing_lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N97x_24tJNs0",
        "outputId": "642d7d7b-16bf-4c30-a3ba-c1571ee5efa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi Lakshmi, how are you?', 'The climate is rainy here.', 'It will be difficult for an outing today.', 'See you later!']\n"
          ]
        }
      ],
      "source": [
        "# Natural Language Tool Kit( NLTK) is a powerful language in Python\n",
        "import nltk \n",
        "\n",
        "#abstract class for default sentence tokenizer\n",
        "nltk.download('punkt') \n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"\"\"Hi Lakshmi, how are you? The climate is rainy here. It will be difficult for an outing today. See you later!\"\"\"\n",
        "tokenized_text=sent_tokenize(text)\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAkIbCNTKvau",
        "outputId": "aa7c777f-f402-4431-f61f-3706e3b405a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'Lakshmi', ',', 'how', 'are', 'you', '?', 'The', 'climate', 'is', 'rainy', 'here', '.', 'It', 'will', 'be', 'difficult', 'for', 'an', 'outing', 'today', '.', 'See', 'you', 'later', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(tokenized_word)\n",
        "print(fdist)\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLUsTUOFLtJc",
        "outputId": "ea1fd8ee-b978-45c5-81fe-cc6f2ce58588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 24 samples and 26 outcomes>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'you': 2, '.': 2, 'Hi': 1, 'Lakshmi': 1, ',': 1, 'how': 1, 'are': 1, '?': 1, 'The': 1, 'climate': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdistCommon = fdist.most_common(5)\n",
        "fdistCommon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O1QJVFwMz0X",
        "outputId": "945fd396-9f72-48b1-9dfd-e95f0f8b884e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('you', 2), ('.', 2), ('Hi', 1), ('Lakshmi', 1), (',', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating list of stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print('stop words : ',stop_words)\n",
        "\n",
        "#REmoving stopwords\n",
        "filtered_sent=[]\n",
        "for w in tokenized_word:\n",
        "    if w not in stop_words:\n",
        "        filtered_sent.append(w)\n",
        "print(\"Tokenized Sentence:\",tokenized_word)\n",
        "print(\"Filterd Sentence:\",filtered_sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjBCKVd1OgfO",
        "outputId": "c7b0f9fa-e9d8-4a71-ec71-e67c784f69bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop words :  {'she', 'it', \"should've\", 'below', 'there', 'shan', 'of', 'can', 'a', 'the', 'that', 'what', 'and', \"you're\", 'he', 'hadn', 'after', 'are', 'an', 'our', 'then', 've', 'off', 'under', \"wasn't\", 'does', 'to', \"shouldn't\", 'ourselves', 'above', 'while', 'theirs', 'by', 'again', 'her', 'each', 'all', 'until', \"mightn't\", 'than', 'here', \"wouldn't\", 'through', \"that'll\", 'who', 'be', 'up', 'only', 'few', \"isn't\", 'own', 'ain', \"you've\", 'out', 'any', 'no', 'is', 'which', 'themselves', 'haven', \"won't\", 'doing', 'being', 'isn', 'or', 'against', 'hasn', 'didn', \"she's\", 'because', 'some', 'into', 'having', 'same', 'they', 'o', \"needn't\", 'how', \"you'll\", 'between', 'himself', 'now', 'shouldn', 'during', 'why', 'won', \"hadn't\", 'further', 'such', 'aren', 'myself', 'other', 'hers', 'wouldn', \"couldn't\", 'very', 'am', 'when', 'have', 'needn', 'my', 'just', 'his', 'ours', \"hasn't\", \"aren't\", 'nor', \"weren't\", 'from', 'over', 'those', 'd', 'ma', 'its', 'you', 'do', 'at', 're', 'herself', 'not', 'but', 'in', 'too', 'on', 'll', 'both', 'couldn', 'itself', 'been', 'most', 'for', 'these', 'once', 'wasn', \"haven't\", 't', 'so', 'their', 'm', 'weren', \"you'd\", 'yours', 'him', 'down', 'them', 'was', 'as', 'this', 'mustn', 'i', 'with', 'y', 'more', 'me', 'your', 'doesn', 'were', 'where', \"mustn't\", 'will', 'about', 'did', 'we', 'if', 's', \"doesn't\", 'don', \"shan't\", \"don't\", \"it's\", \"didn't\", 'had', 'has', 'before', 'yourselves', 'whom', 'mightn', 'should', 'yourself'}\n",
            "Tokenized Sentence: ['Hi', 'Lakshmi', ',', 'how', 'are', 'you', '?', 'The', 'climate', 'is', 'rainy', 'here', '.', 'It', 'will', 'be', 'difficult', 'for', 'an', 'outing', 'today', '.', 'See', 'you', 'later', '!']\n",
            "Filterd Sentence: ['Hi', 'Lakshmi', ',', '?', 'The', 'climate', 'rainy', '.', 'It', 'difficult', 'outing', 'today', '.', 'See', 'later', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# \n",
        "nltk.download('omw-1.4')\n",
        "#\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        " print(\"Lemmatized Word:\",lemmatizer.lemmatize('studies',\"v\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stem = PorterStemmer()\n",
        "print(\"Stemmed Word:\",stem.stem('studies',\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0xlYHJ4Pmj8",
        "outputId": "7d71e4d1-41ab-444b-88e9-ae9a594a1e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Word: study\n",
            "corpora : corpus\n",
            "Stemmed Word: studi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words=[]\n",
        "Lematized_words=[]\n",
        "for w in tokenized_word:\n",
        "    Lematized_words.append(lemmatizer.lemmatize(w,\"v\"))\n",
        "    stemmed_words.append(stem.stem(w,\"v\"))\n",
        "\n",
        "print(\"Tokenized Sentence:\",tokenized_word)\n",
        "print(\"Lematized words:\",Lematized_words)\n",
        "print(\"stemmed words:\",stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ner6m-GrTFD0",
        "outputId": "f2bdb952-765e-4631-c8fc-78b38a217310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['Hi', 'Lakshmi', ',', 'how', 'are', 'you', '?', 'The', 'climate', 'is', 'rainy', 'here', '.', 'It', 'will', 'be', 'difficult', 'for', 'an', 'outing', 'today', '.', 'See', 'you', 'later', '!']\n",
            "Lematized words: ['Hi', 'Lakshmi', ',', 'how', 'be', 'you', '?', 'The', 'climate', 'be', 'rainy', 'here', '.', 'It', 'will', 'be', 'difficult', 'for', 'an', 'out', 'today', '.', 'See', 'you', 'later', '!']\n",
            "stemmed words: ['hi', 'lakshmi', ',', 'how', 'are', 'you', '?', 'the', 'climat', 'is', 'raini', 'here', '.', 'it', 'will', 'be', 'difficult', 'for', 'an', 'outing', 'today', '.', 'see', 'you', 'later', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = 'walking, gone, amazed, went, talked'\n",
        "tokenized_word_2=word_tokenize(text_2)\n",
        "\n",
        "print(\"Tokenized Sentence:\",tokenized_word_2)\n",
        "stemmed_words_2=[]\n",
        "Lematized_words_2=[]\n",
        "for w in tokenized_word_2:\n",
        "    Lematized_words_2.append(lemmatizer.lemmatize(w,\"v\"))\n",
        "    stemmed_words_2.append(stem.stem(w,\"v\"))\n",
        "print(\"Lematized words:\",Lematized_words_2)\n",
        "print(\"stemmed words:\",stemmed_words_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyO7kh9-V3RM",
        "outputId": "75afd088-1ed1-4183-ab68-05b0239b8f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['walking', ',', 'gone', ',', 'amazed', ',', 'went', ',', 'talked']\n",
            "Lematized words: ['walk', ',', 'go', ',', 'amaze', ',', 'go', ',', 'talk']\n",
            "stemmed words: ['walk', ',', 'gone', ',', 'amaz', ',', 'went', ',', 'talk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbOfEAtkW-1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}